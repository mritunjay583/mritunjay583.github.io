'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href','section']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/docs/docs/',title:"Docs",section:"Docs",content:"Section #  Section renders pages in section as definition list, using title and description.\nExample #  {{\u0026lt; section \u0026gt;}}   Developer  Section # Section renders pages in section as definition list, using title and description. Example # {{\u0026lt; section \u0026gt;}} Db-Design-Solution Please read USE CASES and PROBLEM ANALYSIS of DB DESIGN before reading this. Partitioning Db Design Solutioning HLD # Solution in 1 liner: Global Indexes in a combination to hash-partitioned tables # Terminologies # partitioned -\u0026gt; The partitioned refers to the table is partitioned, mostly with hash based partitioning global -\u0026gt; Global means non-partitioned table primarily made to access it\u0026rsquo;s indexes db-design-use-case-problem-analysis Project Use Cases # The project is intended for creation and adoption of Datamarts in the org.   Usage Guidelines  Section # Section renders pages in section as definition list, using title and description. Example # {{\u0026lt; section \u0026gt;}} create index in dmdb Using the following doc requires you the name of the table. If you don\u0026rsquo;t know the tablename here\u0026rsquo;s a doc through which you can find out here - # navigation-around-views-and-tables after reading this. List Already Existing Index on a tables # -- For listing all indexes on all partititioned table name select * from pg_indexes where schemaname = 'partitioned'; -- For listing all indexes on all standalone table name select * from pg_indexes where schemaname = 'standalone'; -- For listing all indexes on package__wbn__latest_prtd partititioned table select * from pg_indexes where schemaname = 'partitioned' and tablename = 'package__wbn__latest_prtd'; -- For listing all indexes on package_scan__wbn_cs_uid__latest_prtd partititioned table select * from pg_indexes where schemaname = 'standalone' and tablename = 'facility__facility_code__latest_std'; To create index # Convention 1 of index naming [DEFAULT] : datamart creation guidelines DataMarts Creation/Updation in DMDB # Why use datamarts and not access data directly with full SQL from application (the widely/mostly known acceptable way) # Reusability - With each metric anyone writes it is contributed to Org and will be available to use for others.   "}),a.add({id:1,href:'/docs/docs/developer/',title:"Developer",section:"Docs",content:"Section #  Section renders pages in section as definition list, using title and description.\nExample #  {{\u0026lt; section \u0026gt;}}   Db-Design-Solution  Please read USE CASES and PROBLEM ANALYSIS of DB DESIGN before reading this. Partitioning Db Design Solutioning HLD # Solution in 1 liner: Global Indexes in a combination to hash-partitioned tables # Terminologies # partitioned -\u0026gt; The partitioned refers to the table is partitioned, mostly with hash based partitioning global -\u0026gt; Global means non-partitioned table primarily made to access it\u0026rsquo;s indexes   db-design-use-case-problem-analysis  Project Use Cases # The project is intended for creation and adoption of Datamarts in the org. As by definition data mart is a subset of the data warehouse(datalake) and is usually oriented to a specific business line or team. We are moving towards a pattern where people will have access to data limited by tables,columns and even rows (by filtering). For most requirements people won\u0026rsquo;t have access to raw data instead they\u0026rsquo;ll access to aggregated data with limited columns.   documentation publish  important links # https://github.com/spring-projects/spring-boot\\ https://github.com/jenkinsci/docker https://github.com/kubernetes-client/java/wiki https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/CONTRIBUTING.md https://github.com/outline/outline https://github.com/chrisleekr/binance-trading-bot/wiki https://github.com/chenglou/react-motion https://github.com/invertase/react-native-firebase (can host pages in git hub for each module) https://github.com/docker/docker.github.io https://github.com/attardi/wikiextractor/wiki/File-Format https://angular-calendar.com/docs/index.html https://github.com/compodoc/compodoc https://github.com/angular-ui/angular-google-maps https://github.com/srigumm/dotnetcore-kafka-integration https://github.com/ivangfr/spring-cloud-stream-kafka-elasticsearch https://github.com/kaiwaehner/ksql-fork-with-deep-learning-function https://github.com/AOSPA/android_kernel_xiaomi_laurel_sprout https://github.com/Apicurio/apicurio-registry/tree/master/docs https://github.com/cloudhut/kowl/tree/master/docs https://github.com/rapid7/metasploit-framework/tree/04e8752b9b74cbaad7cb0ea6129c90e3172580a2/documentation https://github.com/ansible/ansible/tree/devel/docs https://github.com/treasure-data/prestogres#configuration https://github.com/prmr/JetUML https://github.com/remoteintech/remote-jobs/tree/master/site https://github.com/delta-io/delta https://github.com/apache/apisix https://github.com/MuntashirAkon/AppManager https://github.com/ray-project/ray/tree/master/doc https://github.com/cockroachdb/cockroach/tree/master/docs https://github.com/pingcap/tidb https://github.com/strapdata/elassandra/tree/v6.8.4-strapdata/docs https://github.com/scylladb/scylla/tree/master/docs https://github.com/great-expectations/great_expectations/tree/develop/docs https://github.com/pwr-Solaar/Solaar/tree/master/docs https://github.com/Parsely/pykafka https://github.com/thenewboston-developers/thenewboston-python-client https://github.com/joke2k/faker/tree/master/docs https://github.com/confluentinc/ksql/tree/master/docs (mk docs) https://github.com/confluentinc/confluent-kafka-python/tree/master/docs https://github.com/eulerto/pg_similarity https://github.com/orafce/orafce https://github.com/phoronix-test-suite/phoronix-test-suite https://github.com/atlanhq/camelot https://github.com/blackjack4494/yt-dlc https://github.com/brndnmtthws/conky/tree/main/doc https://github.com/gorhill/uBlock https://github.com/facebook/rocksdb/tree/master/docs https://github.com/ether/etherpad-lite/tree/develop/doc https://github.com/facebook/zstd/tree/dev/doc https://github.com/starburstdata/trino/tree/master/docs https://github.com/CZ-NIC/knot-resolver/tree/master/doc https://github.   outcomes limitations support  Outcomes # package_meta - Standard package data Used when filter on action_date (epoch in microseconds) Use of action_date column as a preliminary first-level basic filter is required \u0026amp; mandatory. package_multidimensional - Multi-dimensional package data which is supposed to be used only when action_date filter is NOT possible. Used to query package data using ANY BUT AT LEAST one of the following - cs.   Roadmap  Roadmap with Priority , Effort \u0026amp; Timeline # Objective Priority Effort Timeline Status Deadlock Prevention Logic P2 3d 30 May Refactoring of configuration main P2 2d 15 May New Integration Document P3 1d 12 May In progress Health Check API P0 5d 7 May In progress Architecture: * Exploration Documents * Spark documentation - P1 * Videos Links P1 10d Jun Application tuning for throughput increase P3 3d Jun Addition of meta columns like topic_name, offset, partitions etc.   "}),a.add({id:2,href:'/docs/docs/usage-guidelines/',title:"Usage Guidelines",section:"Docs",content:"Section #  Section renders pages in section as definition list, using title and description.\nExample #  {{\u0026lt; section \u0026gt;}}   create index in dmdb  Using the following doc requires you the name of the table. If you don\u0026rsquo;t know the tablename here\u0026rsquo;s a doc through which you can find out here - # navigation-around-views-and-tables after reading this. List Already Existing Index on a tables # -- For listing all indexes on all partititioned table name select * from pg_indexes where schemaname = 'partitioned'; -- For listing all indexes on all standalone table name select * from pg_indexes where schemaname = 'standalone'; -- For listing all indexes on package__wbn__latest_prtd partititioned table select * from pg_indexes where schemaname = 'partitioned' and tablename = 'package__wbn__latest_prtd'; -- For listing all indexes on package_scan__wbn_cs_uid__latest_prtd partititioned table select * from pg_indexes where schemaname = 'standalone' and tablename = 'facility__facility_code__latest_std'; To create index # Convention 1 of index naming [DEFAULT] :   datamart creation guidelines  DataMarts Creation/Updation in DMDB # Why use datamarts and not access data directly with full SQL from application (the widely/mostly known acceptable way) # Reusability - With each metric anyone writes it is contributed to Org and will be available to use for others. Numerous users do not have to write same query again. Standard Of metrics definition - When a metric calculation in a single way Org wide, it will be easier to standardize the definition for that metric.   dmdb usage  Definition of Datamart (dm) # Datamart is a subset / aggregation / filtered / limited by columns of Raw. Is DMDB right for you? (Use-cases) # Understanding the raw dataset naming convention # Convention: Latest Based Table - {schema_name}{naming_convention}.{table_name}{primary_key_column_list_on_which_the_latest_is_taken_on}{method}{column_name_convention} eg. Package Scan Latest - raw_u.package_scan__wbn_cs_uid__latest_u Append Only Table - {schema_name}{naming_convention}.{table_name}{column_name_convention} eg. Audit Scan Append - raw_u.audit_scan_u Quering data Guidelines # Always action_date as the first filter and never forget to put typecast of bigint in the end.   metric table design  Problem Statement (In RTDB) # The current problem in RTDB is that every metric (i.e. data of every widget) is served by a dedicated table Maintenance overhead of these tables like bloat, deletion etc Privelege control management of these metrics tables are not done. Everything is in under public schema and everyone can access every metric Redundancy of metrics i.e. same (similar/overlapping metrics) are calcualated from the very start No structure of creating aggregations over aggregations Structure of Serving Metrics # We have categorized the metrics in 2 parts - Day before Yesterday Metrics and even Before - MATERIALIZED in a table (aka Historical) CURRENT_DAY and PREVIOUS_DAY (IST) of Metrics - NOT MATERIALIZED - LIVE QUERY ONLY (aka recent) - Why materialization of metrics is required?   navigation-around-views-and-tables  Query to get table name from view name # -- referenced_table_name will contain the tablename SELECT * FROM (SELECT u.view_schema AS schema_nameD, u.view_name, u.table_schema AS referenced_table_schema, u.table_name AS referenced_table_name, v.view_definition FROM information_schema.view_table_usage u JOIN information_schema.views v ON u.view_schema = v.table_schema AND u.view_name = v.table_name WHERE u.table_schema NOT IN ('information_schema', 'pg_catalog') --- CHANGE VIEW NAME HERE AND u.view_name= '{view_name}' ORDER BY u.view_schema, u.view_name) AS VIEW_DETAILS; -- Source https://dataedo.com/kb/query/postgresql/list-tables-used-by-a-view Query to get move around in partition table tree hierarchy # SELECT * FROM pg_partition_root(table_name); SELECT * FROM pg_partition_ancestors(table_name); SELECT * FROM pg_partition_tree(table_name);   Offset management  Spark-Structured Streaming Offset Management APIs # spark-structured streaming has it\u0026rsquo;s own offset management it does not support storing consumed offset information in consumer group in kafka. It uses checkpoint location which you provide. we are using s3 as checkpoint location. Currently there is no APIs in spark-structured streaming using which we can get information of currently consumed offsets or reset offsets for current pipeline. We created APIs which will do all of this work.   programmatic access and Integration  Guidelines for programmatic Integration with DMDB # Use only the user provided to you for your specific application. Even if you have any other credentials like developers credentials, some other application\u0026rsquo;s credentials, etc.; dont use them. Ask for new credentials for each new application/dashboard. Connection Timeout (300ms) - Use this while making the connection to DB. If the connection fail within the timeout move further, log error (raise alarms).   "}),a.add({id:3,href:'/docs/docs/usage-guidelines/create-indexes-in-dmdb-guidlines/',title:"create index in dmdb",section:"Usage Guidelines",content:"Using the following doc requires you the name of the table. If you don\u0026rsquo;t know the tablename here\u0026rsquo;s a doc through which you can find out here - #   navigation-around-views-and-tables after reading this.\n List Already Existing Index on a tables #  -- For listing all indexes on all partititioned table name select * from pg_indexes where schemaname = 'partitioned'; -- For listing all indexes on all standalone table name select * from pg_indexes where schemaname = 'standalone'; -- For listing all indexes on package__wbn__latest_prtd partititioned table select * from pg_indexes where schemaname = 'partitioned' and tablename = 'package__wbn__latest_prtd'; -- For listing all indexes on package_scan__wbn_cs_uid__latest_prtd partititioned table select * from pg_indexes where schemaname = 'standalone' and tablename = 'facility__facility_code__latest_std'; To create index #  Convention 1 of index naming [DEFAULT] :\n{table_name_without_schema}_{list_of_underscore_column_name_with_underscore_replacing_dot_if_col_name_contains_dot}_{type_of_index}_idx\n-- Example to create index on wbn in partitioned table CREATE INDEX package__wbn__latest_prtd_cs_sd_btree_idx ON partitioned.package__wbn__latest_prtd USING btree (\u0026quot;cs.sd\u0026quot;) -- Example to create index on wbn in partitioned table CREATE INDEX package_scan__wbn_cs_uid__latest_prtd_cs_sd_btree_idx ON partitioned.package_scan__wbn_cs_uid__latest_prtd USING btree (\u0026quot;cs.sd\u0026quot;) -- Example to create index on property.facility.facility_type in standalone table CREATE INDEX CONCURRENTLY facility__facility_code__latest_std_facility_type_btree_idx ON standalone.facility__facility_code__latest_std USING btree (\u0026quot;property.facility.facility_type\u0026quot;) Convention 2 of index naming when names get longer than 63 characters [\u0026gt;63 chars] : {table_name_without_schema_without_double_underscored_embraced_column_names}_{list_of_underscore_column_name_with_underscore_replacing_dot_if_col_name_contains_dot}_{type_of_index}_idx\n-- Example when index name exceeds the allowed length and is being truncated. Use this convention CREATE INDEX package_scan_v1_latest_prtd_action_date_cs_slid_btree_idx ON partitioned.package_scan_v1__wbn_cs_uid__latest_prtd USING btree (\u0026quot;action_date\u0026quot;,\u0026quot;cs.slid\u0026quot;) Notes #   The index has to be created on the actual table and not on the raw_* view Always try to create indexes CONCURRENTLY first in case the table is in standalone schema, if the query fails then try removing the concurrently For partitioned schema, do not use CONCURRENTLY in index creation query Use DatamartDB Special to create indexes. The only \u0026ldquo;DatamartDB\u0026rdquo; will not allow to create indexes. However, for all read queries, the only \u0026ldquo;DatamartDB\u0026rdquo; is mandatory. Use the appropriate index type. The btree index is not good for all use-cases. Refer documentation https://www.postgresql.org/docs/current/indexes-types.html Only create index when you need and understand what/why you are doing.  "}),a.add({id:4,href:'/docs/docs/usage-guidelines/datamarts-creations-guidelines/',title:"datamart creation guidelines",section:"Usage Guidelines",content:"DataMarts Creation/Updation in DMDB #  Why use datamarts and not access data directly with full SQL from application (the widely/mostly known acceptable way) #   Reusability - With each metric anyone writes it is contributed to Org and will be available to use for others. Numerous users do not have to write same query again. Standard Of metrics definition - When a metric calculation in a single way Org wide, it will be easier to standardize the definition for that metric. Democratization -  Security  Auditing - Who is currently accessing what. Is that allowed? Logging - Logging of database errors with User level information to identify the wrongdoers fast Logical isolation - Isolation of schemas (aka namespaces) shows the user only those datasets that the user is authorized to do so   Privilege Control - Who has access to what datamarts, how many columns    Schemas in Datamarts that users should be aware of #    raw_u (aka base datamart) - It contains the data at raw level, all columns, unfiltered. The _u means the columns names are separated by undescore.\nExample cs_ud, cs_sd The columns which already has underscore in them stays the same. Eg. trip_uuid\n  raw_d (aka base datamart) - It contains the data at raw level, all columns, unfiltered. The _d means the columns names are separated by dot.\nExample cs.ud, cs.sd, action_date, The dot convention is usually for data which comes from mongoDB.\n  dm (aka datamart / intermediate datamart) - It contains all the datamarts that the dmdb has and is supposed to be the primary knowledgebase of datamart. This is the schema which will be visible in Data Catalog. Every datamart is either getting data from raw* or from another datamart from the same schema.\n  {user_schema} eg. stm, eye (aka exposed datamart) - It\u0026rsquo;s the schema which is visible to the same named user. This schema is exposed to application/backend/dashboard and should be queried by connecting to database programmatically with the username and password. Eg The stm user will query data for stm dashboard and only stm schema is visible to that user.\nWhy we need the {user_schema} -\n It gives control on whom can data access how much data from dm schema. Logical layer separation gives debuggability with better audit logging When the underlying datamart changes its definition, this layer protects the stm user query by providing backward compatibility, control on number of columns, order of columns etc. so no application code changes/releases. Reduces the error vulnerability since the stm user can only access datamarts under it\u0026rsquo;s own schema only Ability to customize or tweak the metric specifically for a particular dashboard    metrics - This is schema which stores all the pre-calculated metrics of historical data. Only materialized historical metrics are suppposed to be stored.\n  Datamart Access Interfaces in dm schema and {user_schema} #  For every widget there has to be TWO datamart access interfaces -\n  LIVE datamart access interface -\n It is based on runtime calculation/computation of the logic/query. Since this executed eachtime, it is supposed to be used for only a small window of data to get output in milliseconds. Metrics of all any time can be queries via this including yesterday, today, current_hour and even current minute.    MATERIALIZED datamart access interface -\n It is based on pre-calculated metrics stored in database. It queries already calculated metrics from a metric table by providing a range of metric interval. There are no limitations on range of metrics. Eg. We can get a metric with with a range of 1 year to make a quarter-on-quarter trend Metrics are available of DAY BEFORE YESTERDAY and BEFORE i.e. (\u0026lt;T-1/\u0026lt;=T-2). In other words metrics of today and yesterday are not available in this.  The metrics is calculated at day level of previous day. So each day, the previous day\u0026rsquo;s metrics is calculated and stored. We cannot the previous day metric too because, we don\u0026rsquo;t know when that metric is scheduled. It could be at 1:00, 5:00, 11:00, 14:00 or whatever. 1 full day is reserved for the metric to get calculated and get saved.      Convention naming for LIVE datamart interfaces #  For dm schema -\n dm.{metric_name}__for_{entity}__for_{duration}\n Examples:\ndm.fdds_cod__for_facility_ids__for_day('{IND148101AAA, INHPAADY}',CURRENT_DATE) dm.fdds_cod__for_client_ids__for_day('{AMAZON, SNAPDEAL_SURFACE}','2020-02-25') For {user_schema} schema -\n user_schema}.{user_schema}__{metric_name}__for_{entity}__for_{duration}\n Example:\nstm.stm__fdds_cod__for_facility_ids__for_day('{IND148101AAA, INHPAADY}',CURRENT_DATE) Convention naming for MATERIALIZED datamart interfaces #  For {user_schema} schema -\n {user_schema}.{user_schema}__{metric_name}__for_{entity}__for_range__at_day_level {user_schema}.{user_schema}__{metric_name}__for_{entity}__for_range__for_granularity\n  Examples:\n stm.stm__fdds_cod__for_facility_ids__for_range__at_day_level({'IND148101AAA, INHPAADY'},'2020-03-25','2020-03-28') stm.stm__fdds_cod__for_facility_ids__for_range__for_granularity({'IND148101AAA, INHPAADY'},'2020-03-25 12:00:00','2020-03-28 13:00:00','1m') Creating the datamarts - (Working Examples) #  LIVE #  Creating the datamart in dm schema -\n--BOILER_PLATE_CODE CREATE OR REPLACE FUNCTION dm.fdds_cod__for_facility_ids__for_day(facility_ids_filter text[],ist_date_filter DATE) RETURNS SETOF RECORD AS $func$ DECLARE utc_timestamp_filter timestamp := ist_date_filter - interval '330' MINUTE; BEGIN RETURN QUERY -- YOUR_QUERY_STARTS_HERE -- explain analyze SELECT * FROM ( SELECT DATE (fadt) dt, \u0026quot;cs_slid\u0026quot;, count(wbn) volume, sum(CASE WHEN ( ss = 'Delivered' AND DATE (fadt) = DATE (ldd) ) THEN 1 ELSE 0 END) fdd FROM ( SELECT cs_slid, wbn, mwn, date_fadt, dd_dct dct, cs_sd sd, dd_fdd, cs_ss ss, (date_fadt + interval '330' MINUTE) fadt, (ldd + interval '330' MINUTE) ldd, row_number() OVER ( PARTITION BY wbn ORDER BY action_date DESC ) AS ROW FROM ( SELECT cs_slid, wbn, mwn, date_fadt, dd_dct, cs_sd, dd_fdd, cs_ss, ldd, pt, action_date, cnid, cl FROM raw_u.package_scan__wbn_cs_uid__latest_u WHERE action_date \u0026gt;= cast(extract(epoch FROM ((date_trunc('day', (utc_timestamp_filter + interval '330' MINUTE)) - interval '0' DAY) - interval '330' MINUTE)) * 1000000 AS BIGINT) AND action_date \u0026lt; cast(extract(epoch FROM ((date_trunc('day', (utc_timestamp_filter + interval '330' MINUTE)) + interval '1' DAY) - interval '330' MINUTE)) * 1000000 AS BIGINT) AND \u0026quot;cs_slid\u0026quot; = any (facility_ids_filter) ) p WHERE cnid = \u0026quot;cs_slid\u0026quot; AND cl NOT IN ('DELHIVERY INTERCHANGE B2B', 'DUMMY B2B', 'DLV Internal Test', 'WalletTest3', 'Delhivery E POD', 'Delhivery', 'Delhivery E POD', 'DEL LS', 'FTPL XB EXPRESS', 'SNAPDEALXB EXPRESS', 'URBANIC XB EXPRESS', 'PRICE DROP XB EXPRESS', 'ANSERX XB B2B', 'CHUMXB EXPRESS') AND cs_sd \u0026gt;= ((utc_timestamp_filter + interval '330' MINUTE)::DATE - interval '0' DAY) - interval '330' MINUTE AND pt IN ('COD') ) AS REF WHERE ROW = 1 AND ( mwn IS NULL OR mwn = wbn ) AND date_fadt \u0026gt;= ((utc_timestamp_filter + interval '330' MINUTE)::DATE - interval '0' DAY) - interval '330' MINUTE GROUP BY cs_slid, DATE (fadt) ) fds; --BOILER_PLATE_CODE END; $func$ LANGUAGE plpgsql VOLATILE; Creating the exposed datamart in stm schema by accessing the above datamart\n--BOILER_PLATE_CODE CREATE OR REPLACE FUNCTION stm.stm__fdds_cod__for_facility_ids__for_day(facility_ids_filter text[],date_filter DATE) RETURNS TABLE (dt DATE,cs_slid text,volume bigint,fdd bigint) AS $func$ #variable_conflict use_column BEGIN RETURN QUERY -- YOUR_QUERY_STARTS_HERE SELECT dt,cs_slid,volume,fdd from dm.fdds_cod__for_facility_ids__for_day(facility_ids_filter,date_filter) f(dt DATE,cs_slid text,volume bigint,fdd bigint); --BOILER_PLATE_CODE END; $func$ LANGUAGE plpgsql VOLATILE; MATERIALIZED #  For creation of datamarts based on materialized datamart access interface refer metric_table-and-materialized-datamarts\nQueries from Application Code with stm user - #  LIVE\n-- Accessing Today's Metric for facitlities - IND148101AAA, INHPAADY, IN152026A1C SELECT dt,cs_slid,volume,fdd from stm__fdds_cod__for_facility_ids__for_day('{IND148101AAA, INHPAADY, IN152026A1C}',CURRENT_DATE) -- Accessing Yesterday's Metric for facitlities - IND148101AAA, INHPAADY, IN152026A1C SELECT dt,cs_slid,volume,fdd, trunc(((fdd::float4/volume::float4)*100)::numeric,2) as \u0026quot;fdd%\u0026quot; from stm__fdds_cod__for_facility_ids__for_day('{IND148101AAA, INHPAADY, IN152026A1C}',CURRENT_DATE-1); -- Accessing 2021-04-07 Metric for facitlities - IND148101AAA, INHPAADY, IN152026A1C SELECT dt,cs_slid,volume,fdd from stm__fdds_cod__for_facility_ids__for_day('{IND148101AAA, INHPAADY, IN152026A1C}','2021-04-07') MATERIALIZED\n-- Accessing Metric of range 2021-04-05 to 2021-04-07 for facitlities - IND148101AAA, INHPAADY, IN152026A1C SELECT date_value,facility_id,total as volume,fdds as fdd from stm.stm__fdds_cod__for_facility_ids__for_range__at_day_level('{IND148101AAA, INHPAADY, IN152026A1C}',CURRENT_DATE-10,CURRENT_DATE-1) COMBINED\n-- Accessing Metric Last 10 days for facitlities - IND148101AAA, INHPAADY, IN152026A1C SELECT date_value,facility_id,sum(volume) as volume,sum(fdd) as fdd, trunc(((sum(fdd)::float4/sum(volume)::float4)*100)::numeric,2) as \u0026quot;fdd%\u0026quot; from ( SELECT dt as date_value,cs_slid as facility_id,volume,fdd from stm__fdds_cod__for_facility_ids__for_day('{IND148101AAA, INHPAADY, IN152026A1C}',CURRENT_DATE-1) UNION ALL --# TODO -- This typecast into bigint should not be required since function will already do it inside SELECT date_value,facility_id,total::bigint as volume,fdds::bigint as fdd from stm.stm_fdds_cod__for_facility_ids__for_range__at_day_level('{IND148101AAA, INHPAADY, IN152026A1C}',CURRENT_DATE-10,CURRENT_DATE-1) ) fv group by 1,2  Note: Schema name stm. is not required i.e. when the user access its own named schema\n Creating the datamart for Datacatalog (catalog.delhivery.com) #  The datacatalog would require views for displaying sameple/dummy data from a datamart. So a view should be created by datamart authors to list it into datacatalog. The datamart authors also have to write datamart description, column description etc. in datacatalog.\nCREATE OR replace view dm.b2b_center_connect as select * from stm.b2b_center_connect_for_facility_ids((select array_agg(property_facility_facility_code) from raw_u.facility__facility_code__latest_u),CURRENT_DATE); Why FUNCTIONS? and NOT VIEWS for database ? #    Views are limited when passing filter arguments like facility_ids, client_ids in query while putting WHERE clause while querying the views.\nSince postgres re-writes every query it pushes this filter parameter inside and filters at the initial level. However, this doesn\u0026rsquo;t happen everytime specially the queries in which we do a window function filter or complex aggregates. If postgres doesn\u0026rsquo;t understand to put the filter in the initial level, then it calculates the result for all facilties/clients and then later on filters the output leading to increased response times.\n  Increasing Vulnerability and decreasing enforcement on how to query datamart - If we use views it is upto the user that it specifies the facilities_ids/clients_ids. A user is free to query data of all centers and take all data into application and then filter in application, which is anti-pattern and leads to high execution time.\n  Views makes more problems to update than functions. Whenever we update the column type, order of columns, or number of columns views doesn\u0026rsquo;t allow us to replace on the fly. In functions, with datamarts written in dm schema, they are schemaless so updating them is easier.\n  "}),a.add({id:5,href:'/docs/docs/developer/db-design-solution-hld/',title:"Db-Design-Solution",section:"Developer",content:" Please read USE CASES and PROBLEM ANALYSIS of DB DESIGN  before reading this.\n Partitioning Db Design Solutioning HLD #  Solution in 1 liner: Global Indexes in a combination to hash-partitioned tables #  Terminologies #   partitioned -\u0026gt; The partitioned refers to the table is partitioned, mostly with hash based partitioning\n  global -\u0026gt; Global means non-partitioned table primarily made to access it\u0026rsquo;s indexes\n Every pipeline will require 2 tables namely {pipeline_name}._prtd and {pipeline_name}._gbl\nSample DB Design and Explanation #  Create the schemas #  -- Trying not to create anything in public schema. Trying to keep things clean, and controlled from start create schema partitioned; create schema partitions; create schema subpartitions; create schema global; create schema raw; -- aka base datamart create schema dm; -- aka Intermediate data marts -- create schema eye; -- aka exposed datamart. This should be mapped with username one to one Creating partitioned table of pipeline #  -- Creating a separate schema to distinguish b/w normal tables and partitioned -- Also adds scope for better privelege control create TABLE partitioned.package_prtd ( wbn text, \u0026quot;cs.st\u0026quot; text, \u0026quot;cs.ss\u0026quot; text, cl text, \u0026quot;cs.uid\u0026quot; text, action_date bigint, \u0026quot;cs.sd\u0026quot; timestamp, cd timestamp, event_type text ) PARTITION BY RANGE (action_date); -- Used to get the epochs in microseconds --select (extract(epoch from '2020-12-01'::timestamp)*1000000)::bigint; CREATE TABLE partitions.package_2020_12_01_to_2020_12_05 PARTITION OF partitioned.package_prtd FOR VALUES FROM (1606780800000000) TO (1607126400000000) PARTITION BY HASH(cl); CREATE TABLE partitions.package_2020_12_05_to_2020_12_10 PARTITION OF partitioned.package_prtd FOR VALUES FROM (1607126400000000) TO (1607558400000000) PARTITION BY HASH(cl); -- Make default partition for safety. Ideally when partitioned on action_date this should not be used CREATE TABLE partitions.package_default_partition PARTITION OF partitioned.package_prtd DEFAULT; -- Creating a separate schema for partitions to make sure we don't clutter up the schema user is seeing -- This is supposed to be an internal schema and no user should be interested in seeing this create TABLE subpartitions.package_2020_12_01_to_2020_12_05__00 PARTITION OF partitions.package_2020_12_01_to_2020_12_05 FOR VALUES with (MODULUS 3, REMAINDER 0); create TABLE subpartitions.package_2020_12_01_to_2020_12_05__01 PARTITION OF partitions.package_2020_12_01_to_2020_12_05 FOR VALUES with (MODULUS 3, REMAINDER 1); create TABLE subpartitions.package_2020_12_01_to_2020_12_05__02 PARTITION OF partitions.package_2020_12_01_to_2020_12_05 FOR VALUES with (MODULUS 3, REMAINDER 2); create TABLE subpartitions.package_2020_12_05_to_2020_12_10__00 PARTITION OF partitions.package_2020_12_05_to_2020_12_10 FOR VALUES with (MODULUS 3, REMAINDER 0); create TABLE subpartitions.package_2020_12_05_to_2020_12_10__01 PARTITION OF partitions.package_2020_12_05_to_2020_12_10 FOR VALUES with (MODULUS 3, REMAINDER 1); create TABLE subpartitions.package_2020_12_05_to_2020_12_10__02 PARTITION OF partitions.package_2020_12_05_to_2020_12_10 FOR VALUES with (MODULUS 3, REMAINDER 2); --Change Note: #TODO Improve This -- Partitioning Key =\u0026gt; (action_date, cl, wbn) in above example but on prod the Partitioning Key is =\u0026gt; (action_date,cl,cs.slid,wbn) -- Primary Key cannot be made because of partitioning key can be null. --In above case we are using action_date , cl and wbn out of which no key is null but on prod, since we are have cs.slid it can be null -- So we didn't create any primary key. --Though THe primary key constraint is covered by global table constraint -- required for unique index which is eligible for onflict arg -- Multicolumn partitioning of cl and wbn is thought but requires both to be in query to get pruned. -- Refer [this](https://www.enterprisedb.com/postgres-tutorials/what-multi-column-partitioning-postgresql-and-how-pruning-occurs)  Do not attempt to play on client prefix id as they are broken with aramax integrations. Aramax uses 20-25 client prefix instead of 1\n Creating the global table for pipeline (For global indexes) #  -- This schema shall also contain the function too which is maintaining the sync CREATE TABLE global.package_gbl ( wbn text, cl text, action_date bigint, \u0026quot;cs.sd\u0026quot; timestamp, cd timestamp, PRIMARY KEY(wbn) ); -- Not creating Primary Key Constraint on Global Table since it's not required. -- Assuming global table will always be in sync with partitioned table -- There's also no need for an index because the other indexes should always contain wbn within it -- i.e. always create compound (multicolumn) index eg. (cd_sd,cl,wbn) , (action_date, cl,wbn) -- Sync Functiona CREATE OR REPLACE FUNCTION global.package_prtd_function() RETURNS trigger AS $func$ BEGIN CASE TG_OP WHEN 'INSERT' THEN INSERT INTO global.package_gbl ( wbn,cl,action_date,\u0026quot;cs.sd\u0026quot;,cd) VALUES ( NEW.wbn,NEW.cl,NEW.action_date,NEW.\u0026quot;cs.sd\u0026quot;,NEW.cd ); RETURN NEW; WHEN 'DELETE' THEN DELETE FROM global.package_gbl WHERE wbn = OLD.wbn; RETURN OLD; WHEN 'UPDATE' THEN UPDATE global.package_gbl SET (cl,action_date,\u0026quot;cs.sd\u0026quot;,cd) = (NEW.cl,NEW.action_date,NEW.\u0026quot;cs.sd\u0026quot;,NEW.cd) WHERE package_gbl.wbn = OLD.wbn; RETURN NEW; ELSE RAISE EXCEPTION 'Unhandled Case in Trigger NEW --\u0026gt; % OLD --\u0026gt; %', NEW, OLD; END CASE; END $func$ LANGUAGE plpgsql VOLATILE; -- Trigger which is attached on partitioned table -- Note: Putting a trigger before since insertion should happen in global table to check on constraint -- This is done as a PRIMARY(action_date,cl,wbn) is vulnerable as action_date is a running key. So there's a -- chance that a new action_date with same wbn could get enter in case of an application level bug -- To stop that we'll insert in global table first since it'll have a unique constraint of wbn -- Note: Trigger have to be on leaf partitions as postgres doesn't support BEFORE partitions on parent partitioned table -- Need postgres 13 for creating triggers on partitioned table CREATE TRIGGER package_2020_12_01_to_2020_12_05__00_trigger BEFORE INSERT OR UPDATE OR DELETE on subpartitions.package_2020_12_01_to_2020_12_05__00 FOR EACH ROW EXECUTE PROCEDURE global.package_prtd_function(); CREATE TRIGGER package_2020_12_01_to_2020_12_05__01_trigger BEFORE INSERT OR UPDATE OR DELETE on subpartitions.package_2020_12_01_to_2020_12_05__01 FOR EACH ROW EXECUTE PROCEDURE global.package_prtd_function(); CREATE TRIGGER package_2020_12_01_to_2020_12_05__02_trigger BEFORE INSERT OR UPDATE OR DELETE on subpartitions.package_2020_12_01_to_2020_12_05__02 FOR EACH ROW EXECUTE PROCEDURE global.package_prtd_function(); CREATE TRIGGER package_2020_12_05_to_2020_12_10__00_trigger BEFORE INSERT OR UPDATE OR DELETE on subpartitions.package_2020_12_05_to_2020_12_10__00 FOR EACH ROW EXECUTE PROCEDURE global.package_prtd_function(); CREATE TRIGGER package_2020_12_05_to_2020_12_10__01_trigger BEFORE INSERT OR UPDATE OR DELETE on subpartitions.package_2020_12_05_to_2020_12_10__01 FOR EACH ROW EXECUTE PROCEDURE global.package_prtd_function(); CREATE TRIGGER package_2020_12_05_to_2020_12_10__02_trigger BEFORE INSERT OR UPDATE OR DELETE on subpartitions.package_2020_12_05_to_2020_12_10__02 FOR EACH ROW EXECUTE PROCEDURE global.package_prtd_function(); -- Trigger of default partition CREATE TRIGGER package_default_partition_trigger BEFORE INSERT OR UPDATE OR DELETE on partitions.package_default_partition FOR EACH ROW EXECUTE PROCEDURE global.package_prtd_function(); Bulk Data Generation #  -- Bulk Data Insertion in Partitioned table INSERT INTO partitioned.package_prtd select g+5961810374542, (array['PP','UD','DL','PU','RT'])[floor(random() * 5 + 1)], (array['Manifested','Not Picked','In Transit','Dispatched','Scheduled','Pending','Delivered'])[floor(random() * 7 + 1)], md5((g % 100)::text), md5(g::text), cast (extract ( epoch from ('2020-12-01 14:26:02.960718+00'::timestamp + ( g || ' second' ) :: interval))*1000000 as bigint), '2020-12-01 14:26:02.960718+00'::timestamp + ( g || ' milliseconds' ) :: interval, '2020-12-01 14:26:02.960718+00'::timestamp + ( g || ' second' ):: interval from generate_series(1,1000000) as g; -- This is bulk insertion for package_gbl. Usually not required since -- trigger will make the data available in global table too. -- INSERT INTO global.package_gbl select g+5961810374542,md5(g::text),g*999,CURRENT_TIMESTAMP - ( g || ' second' ) :: interval,CURRENT_TIMESTAMP - ( g || ' second' ) :: interval from generate_series(1,1000000) as g; --Sample INSERT on conflict query --INSERT INTO partitioned.package_prtd values ( -- '7269364714286','DL','RTO','SNAPDEAL SURFACE','scan::1f7d9f0f-670a-48e2-9143-480ed932e3c4','1606309512002468','2020-11-25 13:05:12.407','2020-11-13 12:49:54.19' -- ) --ON CONFLICT (cd,cl,wbn) --DO UPDATE SET --wbn = EXCLUDED.wbn, --\u0026quot;cs.st\u0026quot; = EXCLUDED.\u0026quot;cs.st\u0026quot;, --\u0026quot;cs.ss\u0026quot;= EXCLUDED.\u0026quot;cs.ss\u0026quot;, --cl = EXCLUDED.cl, --\u0026quot;cs.uid\u0026quot; = EXCLUDED.\u0026quot;cs.uid\u0026quot;, --action_date = EXCLUDED.action_date, --\u0026quot;cs.sd\u0026quot; = EXCLUDED.\u0026quot;cs.sd\u0026quot;, --cd = EXCLUDED.cd; Creating the Index on partitioned table #  -- Creating index on wbn only because the partitioning already know the path to right partition so -- this index will be used after reaching the partition in case of update / delete / select create INDEX package_prtd_wbn_btree_idx ON partitioned.package_prtd using btree(wbn) ; -- Another list of index - make more as per needs - but since we know action_Date, cl and cs.slid are most queried -- We'll start with those create INDEX package_prtd_action_date_btree_idx ON partitioned.package_prtd using btree(action_date) ; create INDEX package_prtd_cl_btree_idx ON partitioned.package_prtd using btree(cl) ; create INDEX package_prtd_cs_slid_btree_idx ON partitioned.package_prtd using btree(\u0026quot;cs.slid\u0026quot;) ; Test Queries -\u0026gt;\nexplain analyze select * from partitioned.package_prtd where action_date \u0026gt; 1606832764960717 and action_date \u0026lt; 1606832764960720 and cl='c81e728d9d4c2f636f067f89cc14862c';\nexplain analyze select * from partitioned.package_prtd where action_date \u0026gt; 1606832764960717 and action_date \u0026lt; 1606832764960720 and cl='c81e728d9d4c2f636f067f89cc14862c' and wbn ='5961810374544';\n While it is known, hash indexes are faster for lookups than btree, refer this but they cannot be used in neither in cl nor in wbn because people query somtimes on pattern like for cl '%AMAZON%' and for wbn \u0026lsquo;1337%'\n Indexes on Global Table #  -- We need indexes for lookups of wbn to pick based on range -- Each time user needs a new column as a primary filter it need -- to have in global table and create a global index on it. -- Always create index containing the action_date,cl,wbn so we never go the global table actually -- This index is used for update query create index package_gbl_action_date_cl_wbn_btree_idx on global.package_gbl using btree ( action_date,cl,wbn); -- These index are used primarily for select when action_Date is not the first level priliminary filter create index package_gbl_cs_sd_action_date_cl_wbn_btree_idx on global.package_gbl using btree ( \u0026quot;cs.sd\u0026quot;,action_date,cl,wbn); -- TODO Check if wbn is required create index package_gbl_cd_action_date_cl_wbn_btree_idx on global.package_gbl using btree ( cd,action_date,cl,wbn); create index package_gbl_wbn_action_date_cl_btree_idx on global.package_gbl using btree ( wbn,action_date,cl); create index package_gbl_wbn_action_date_btree_idx on global.package_gbl using btree ( wbn,action_date);  Why Not Brin. See this Not creating a single index on cl because there seems to be no use-case for people to query all data of a cl. Even if it\u0026rsquo;s there they should use the cs.sd / action_date since default partition has always some old records.\n Test Queries of global table(not including plans to limit doc size) #   explain analyze SELECT wbn,cd,cl from global.package_gbl where wbn = '7269364714286'; explain analyze select wbn, \u0026quot;cs.sd\u0026quot; from global.package_gbl gp where gp.\u0026quot;cs.sd\u0026quot; \u0026gt; '2020-12-01 14:16:52.488192' and gp.\u0026quot;cs.sd\u0026quot; \u0026lt; '2020-12-01 14:26:02.988192';  Inserting/Updating Data #  Sample queries - Plain (Not Used Actually - Just to create some data) #  -- Use this to view data with partition name and delete -- select tableoid::regclass,* from package_prtd ; explain analyze INSERT INTO partitioned.package_prtd values ( '7269364714286','DL','RTO','SNAPDEAL SURFACE','scan::1f7d9f0f-670a-48e2-9143-480ed932e3c4','1606780800000006','2020-11-02 13:05:12.407','2020-11-01 12:49:54.19','CREATE' ); INSERT INTO partitioned.package_prtd values ( '7269364714287','DL','RTO','AMAZON','scan::2f7f9ffdf-670a-48e2-9143-6534gdfgfd','1607126400000000','2020-11-03 13:05:12.407','2020-11-01 12:49:54.19','CREATE' ); Real Queries which will run from application #  **INSERT** - -- We'll insert every record after checking if there's no record exist -- Though on-conflict with do nothing covers idempotency but there are other cases too described below explain analyze insert into partitioned.package_prtd (col1,col2,col3) select '7269364714286','DL','RTO','SNAPDEAL SURFACE','scan::1f7d9f0f-670a-48e2-9143-480ed932e3c4','1606780800000006','2020-11-02 13:05:12.407','2020-11-01 12:49:54.19','CREATE' where not exists ( select wbn from global.package_gbl where wbn = '7269364714286' ); **SELECT** -- Getting the current action_date corresponding to wbn which we'll fire update for SELECT wbn,action_date,\u0026quot;cs.slid\u0026quot; from global.package_gbl where wbn in( '7269364714286', '7269364714286'); **UPDATE** -- Update query specifiying the right and exact action_Date with equals so that we prune the partition right update partitioned.package_prtd as p SET \u0026quot;cs.st\u0026quot; = 'RT', action_date = 1611659129000104 WHERE p.action_date = 1611659129000103 -- idompotency and unordered data from source handling and 1611659129000103 \u0026lt; 1611659129000104 and \u0026quot;cs.slid\u0026quot; = 'Gurgaon Central' and p.cl = 'SNAPDEAL SURFACE' and p.wbn = '7269364714286'; -- Using Global Table join (not used in prod) -DEPRECATED explain analyze update partitioned.package_prtd as p SET \u0026quot;cs.st\u0026quot; = 'RT' FROM (SELECT action_date as action_Date from global.package_gbl where wbn = '7269364714286' and cl = 'SNAPDEAL SURFACE' and action_date \u0026lt; 1611659129000103 ) gp WHERE p.action_date = gp.action_date and p.action_date \u0026lt; 1611659129000103 and p.cl = 'SNAPDEAL SURFACE' and p.wbn = '7269364714286'; -- Using only partitioned table ( Scannning knowing all 1st level partitions) (not used in prod) DEPRECATED explain analyze update partitioned.package_prtd as p SET \u0026quot;cs.st\u0026quot; = 'RT' WHERE p.action_date \u0026lt; 1611659129000103 and p.cl = 'SNAPDEAL SURFACE' and p.wbn = '7269364714286'; -- Using subquery (not used in prod) DEPRECATED explain analyze update partitioned.package_prtd as p SET \u0026quot;cs.st\u0026quot; = 'RT' WHERE p.action_date = (SELECT action_date from global.package_gbl where action_date \u0026lt;= 1607558400000000 and cl = 'SNAPDEAL SURFACE' and wbn = '7269364714286' ) and p.cl = 'SNAPDEAL SURFACE' and p.wbn = '7269364714286'; Cases Covered - #    Replay Cases (How to achieve idempotency) and Case when data is unordered from the source itself -\nIn case of insert we are checking if the record exists in global table or not. If not then only we proceed for insertion. In the case of update we are updating the record only when a new action_Date i.e. new action_date greater than existing action_date comes, then only we are updating.\n  Case when data older than 6 months -\nIn the case when we receive a update after 6 months there\u0026rsquo;s a possibility that record is deleted. Because of this case, we always need to run a INSERT before running a UPDATE for every record. That is 2 query sets for 1 record.\n  Case when we don\u0026rsquo;t have event_type key -\nWe don\u0026rsquo;t care about event_type much because we\u0026rsquo;re inserting every record first and then updating it. Though there can be a optimization in future for sources which does send event_type.\n  Combining the partitioned table and global table | Creating the view #  -- Tried merge Join (by using sort by) and using in operator. --The below uses Nested Loop and offers best performance. See [this](https://www.cybertec-postgresql.com/en/join-strategies-and-performance-in-postgresql/) DROP VIEW IF EXISTS raw.package_multidimensional; CREATE OR REPLACE view raw.package_multidimensional as SELECT gp.wbn,gp.\u0026quot;cs.sd\u0026quot;,gp.cd,pp.action_date,pp.cl,pp.\u0026quot;cs.uid\u0026quot;,pp.\u0026quot;cs.st\u0026quot;,pp.\u0026quot;cs.ss\u0026quot; from global.package_gbl gp JOIN partitioned.package_prtd pp ON gp.action_date= pp.action_date AND gp.cl = pp.cl AND gp.wbn = pp.wbn ; DROP VIEW IF EXISTS raw.package_meta; -- TODO remove sensitive columns; CREATE OR REPLACE view raw.package_meta as SELECT * from partitioned.package_prtd; --TODO Make a view with underscore convention for compatibility Selection of columns between partitioned and global table #    cl -\u0026gt; From partitioned table\n We are using partitioned table cl and not global table cl because it\u0026rsquo;s slightly faster in query test plans, the smaller partitioned pkey index is used and in case of direct queries we also have a dedicated index on cl as mentioned above. There\u0026rsquo;s no dedicated index on cl in global table    wbn -\u0026gt; From global table\n specified above combines index of (wbn,cd,cl) was slightly slower than index on just wbn with a sequential scan on cd,cl. Since we have a global wbn index this would give enablement to query only on wbn without any action_date / cd.sd filter    action_Date -\u0026gt; From partitioned table\n Similar to cl we\u0026rsquo;ll prefer cd from partitioned table because it uses partitioned index which will save IO. There\u0026rsquo;s no requirement of dedicated cd index because it has one (package_gbl_cl_wbn_btree_idx) which has cd as first key In partitioned table also we have the main primary key index which has cd in first position    cd and cs.sd -\u0026gt; From global table\n Both the columns have compound indexes starting these column in order The cd and cs.sd in partitioned_table is not indexed and no need to NOT be.    Final Query Test with Partition Pruning #  Queries: - \u0026gt;\nexplain analyze select * from raw.package_multidimensional where action_date \u0026gt; 1606939513960000 and action_Date \u0026lt; 1606939676960718 and cl = 'aab3238922bcc25a6f606eb525ffdc56' ;\nTry same query to pruning off to confirm\nset enable_partition_pruning TO false;  There\u0026rsquo;s only a slight increase because of very low number of partitions with very substantially lesser data.\n Clean Everything #  drop schema partitioned cascade; drop schema partitions cascade; drop schema subpartitions cascade; drop schema global cascade; drop schema raw cascade; drop schema dm cascade; Extra SQL for management / debugging #  -- Add all schemas to search path for use alter role datawarehouse set search_path = \u0026quot;$user\u0026quot;, public, partitioned, subpartitions, partitions, global, raw, dm; -- Show all schemas select schema_name from information_schema.schemata; -- Hierarchy SELECT * FROM pg_partition_root('package_2020_12_01_to_2020_12_05__01'); SELECT * FROM pg_partition_ancestors('package_2020_12_01_to_2020_12_05__01'); SELECT * FROM pg_partition_tree('package_prtd'); https://www.postgresql.org/docs/current/pageinspect.html https://www.postgresql.org/docs/12/pgbuffercache.html can use the enable_seqscan and enable_indexscan parameters Tuning Configs #  * SET enable_partitionwise_aggregate=on; * SET enable_partitionwise_join=on; * SET enable_partition_pruning=on; -- Default is on wal_compression= on https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.html#Appendix.PostgreSQL.CommonDBATasks.Autovacuum #### Possible Optimizations * Slid FIlter in update quuery to prune exactly instead of 3 tables. Blocked by null case * Do SELECT once for INSERT queries and don't send INSERT queries which are not required * TODO -\u0026gt; REview this https://www.enterprisedb.com/blog/row-movement-across-postgresql-partitions-made-easy\n Please read OUTCOMES, LIMITATIONS, SUPPORT after reading this.\n "}),a.add({id:6,href:'/docs/docs/developer/db-design-use-case-problem-analysis/',title:"db-design-use-case-problem-analysis",section:"Developer",content:"Project Use Cases #  The project is intended for creation and adoption of Datamarts in the org. As by definition data mart is a subset of the data warehouse(datalake) and is usually oriented to a specific business line or team. We are moving towards a pattern where people will have access to data limited by tables,columns and even rows (by filtering). For most requirements people won\u0026rsquo;t have access to raw data instead they\u0026rsquo;ll access to aggregated data with limited columns.\nType of Queries #  action_date cs.sd\nwbn - CS (client support) yeh waybill de dedo calibration waybills - sorters (checking waybills)\njoin on bs, trip_id Freshdesk / Escalation - specific wbn with daterange\nAPPLICATION HLD #  TODO:\nDB DESIGN #  Partitioning Consideration #   Prefer partitioning to small partitions size (say 1M to 5M records per partition) because smaller partitions will have smaller tables and indexes (faster lookups, low IO) With smaller indexes it\u0026rsquo;s possible for postgres to keep whole of index in-memory (probably recent ones/more queried) and better manage the memory for indexes, table cache With postgres 12 going close to ~30-60 partitions is a safe bet. See this Partition pruning in extremely^2 important both for query planning time and query execution time. So our partitioning technique should make pruning happen in every query. See this With smaller partitions, we have a chance for sequential reads only and no index scans. See this  Why Partitioning only on created_date is a not so good idea (eg. cd in package) ? #   With cd we won\u0026rsquo;t actually be able to use that because, people can\u0026rsquo;t write query on cd. They need something like action_Date, updated_date, ad etc. When we partition by cd for no real benefit(lets say), we are trading (loosing) something or the other. With cd partitioning if people write query on action_Date, updated_date, ad (assuming we have index on that) we still miss the pruning. Pruning happens only when we query data on partitioned column.  Then, How to partition table supporting upserts with partitioning pruning ?? #  The ideal approach of this problem would have been to partition data on something like action_date, ad or updated_date and moving data between partitions when running UPSERTs. This cannot happen in postgres because -\n There are no global indexes in postgres partitioned table yet. See -\u0026gt; Postgres Proposal Link Since there are no global indexes (i.e. indexes exist per partition ) we cannot create a ON CONFLICT condition globally, so cannot fire a UPDATE which moves data accross partitions.   Please note postgres does support partition movement in UPDATE query. Please don\u0026rsquo;t confuse with limitation of UPSERT command as explained above. See this\n  FAQs\n "}),a.add({id:7,href:'/docs/docs/usage-guidelines/dmdb-usage-guide/',title:"dmdb usage",section:"Usage Guidelines",content:"Definition of Datamart (dm) #  Datamart is a subset / aggregation / filtered / limited by columns of Raw.\nIs DMDB right for you? (Use-cases) #  Understanding the raw dataset naming convention #  Convention:\n Latest Based Table - {schema_name}{naming_convention}.{table_name}{primary_key_column_list_on_which_the_latest_is_taken_on}{method}{column_name_convention} eg. Package Scan Latest - raw_u.package_scan__wbn_cs_uid__latest_u Append Only Table - {schema_name}{naming_convention}.{table_name}{column_name_convention} eg. Audit Scan Append - raw_u.audit_scan_u  Quering data Guidelines #    Always action_date as the first filter and never forget to put typecast of bigint in the end. action_date is the epoch in microseconds. Eg.\n-- Get today's (IST) data and avoid scanning future partitions WHERE action_date \u0026gt;= cast(extract(epoch FROM ((date_trunc('day',(CURRENT_TIMESTAMP + interval '330' MINUTE)) - interval '0' DAY) - interval '330' MINUTE)) * 1000000 AS BIGINT) AND action_date \u0026lt; cast(extract(epoch FROM ((date_trunc('day',(CURRENT_TIMESTAMP + interval '330' MINUTE)) + interval '1' DAY) - interval '330' MINUTE)) * 1000000 AS BIGINT)   Never continue running query which is taking more than 1 minute on DMDB. Expect results within seconds. Always run explain {your_query} to check whether you are scanning only the required partitions if your query is running long.\n  Check for Indexes on tables - Refer doc create-indexes-in-dmdb-guidlines.md\n  Follow the optimization techniques mentioned in https://docs.google.com/presentation/d/1hrtB0cJ3t98CyhoTRKk5EQARXtvAFU38vGdjTgBsEQ0/edit?usp=sharing\n  "}),a.add({id:8,href:'/docs/docs/developer/documentation-publish/',title:"documentation publish",section:"Developer",content:"important links #   https://github.com/spring-projects/spring-boot\\ https://github.com/jenkinsci/docker https://github.com/kubernetes-client/java/wiki https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/CONTRIBUTING.md https://github.com/outline/outline https://github.com/chrisleekr/binance-trading-bot/wiki https://github.com/chenglou/react-motion https://github.com/invertase/react-native-firebase (can host pages in git hub for each module) https://github.com/docker/docker.github.io https://github.com/attardi/wikiextractor/wiki/File-Format https://angular-calendar.com/docs/index.html https://github.com/compodoc/compodoc https://github.com/angular-ui/angular-google-maps https://github.com/srigumm/dotnetcore-kafka-integration https://github.com/ivangfr/spring-cloud-stream-kafka-elasticsearch https://github.com/kaiwaehner/ksql-fork-with-deep-learning-function    https://github.com/AOSPA/android_kernel_xiaomi_laurel_sprout https://github.com/Apicurio/apicurio-registry/tree/master/docs https://github.com/cloudhut/kowl/tree/master/docs https://github.com/rapid7/metasploit-framework/tree/04e8752b9b74cbaad7cb0ea6129c90e3172580a2/documentation https://github.com/ansible/ansible/tree/devel/docs https://github.com/treasure-data/prestogres#configuration https://github.com/prmr/JetUML https://github.com/remoteintech/remote-jobs/tree/master/site https://github.com/delta-io/delta https://github.com/apache/apisix https://github.com/MuntashirAkon/AppManager https://github.com/ray-project/ray/tree/master/doc https://github.com/cockroachdb/cockroach/tree/master/docs https://github.com/pingcap/tidb https://github.com/strapdata/elassandra/tree/v6.8.4-strapdata/docs https://github.com/scylladb/scylla/tree/master/docs https://github.com/great-expectations/great_expectations/tree/develop/docs https://github.com/pwr-Solaar/Solaar/tree/master/docs https://github.com/Parsely/pykafka https://github.com/thenewboston-developers/thenewboston-python-client https://github.com/joke2k/faker/tree/master/docs https://github.com/confluentinc/ksql/tree/master/docs (mk docs) https://github.com/confluentinc/confluent-kafka-python/tree/master/docs https://github.com/eulerto/pg_similarity https://github.com/orafce/orafce https://github.com/phoronix-test-suite/phoronix-test-suite https://github.com/atlanhq/camelot https://github.com/blackjack4494/yt-dlc https://github.com/brndnmtthws/conky/tree/main/doc https://github.com/gorhill/uBlock https://github.com/facebook/rocksdb/tree/master/docs https://github.com/ether/etherpad-lite/tree/develop/doc https://github.com/facebook/zstd/tree/dev/doc https://github.com/starburstdata/trino/tree/master/docs https://github.com/CZ-NIC/knot-resolver/tree/master/doc https://github.com/curl/curl/tree/master/docs   "}),a.add({id:9,href:'/docs/docs/usage-guidelines/metric_table-design-and-usage/',title:"metric table design",section:"Usage Guidelines",content:"Problem Statement (In RTDB) #   The current problem in RTDB is that every metric (i.e. data of every widget) is served by a dedicated table  Maintenance overhead of these tables like bloat, deletion etc   Privelege control management of these metrics tables are not done. Everything is in under public schema and everyone can access every metric Redundancy of metrics i.e. same (similar/overlapping metrics) are calcualated from the very start No structure of creating aggregations over aggregations  Structure of Serving Metrics #   We have categorized the metrics in 2 parts -  Day before Yesterday Metrics and even Before - MATERIALIZED in a table (aka Historical) CURRENT_DAY and PREVIOUS_DAY (IST) of Metrics - NOT MATERIALIZED - LIVE QUERY ONLY (aka recent) -    Why materialization of metrics is required? #  The metrics on historical data (immutable) needs to materialized in a table for the sole purpose of not running the original query again, because we know the isn\u0026rsquo;t going change. For example We calculate a metric at the end of the day for the previous day, we know that since the day is passed no more data can be written for that date so the output of the query will never change. The intent is to reduce the data scan overall.\nThe Materialized Metric Table Schema #  We are creating the metrics per client per facility level to open scope for more diverse aggregations on aggregated metrics. This means we would require to put group by client_id and facility_id in each other (if the columns exist in the table)\nCREATE SCHEMA metrics; CREATE TABLE metrics.metric_registry ( id UUID PRIMARY KEY DEFAULT gen_random_uuid(), created_at timestamp default CURRENT_TIMESTAMP, metric_name text unique not null, created_by_email text not null, created_for text not null, used_by text[] not null, stakeholder_emails text[] not null, -- ## #TODO make this infor available in Atlan description text not null, scheduling_reference_link text, should_run bool ); CREATE TABLE metrics.mat_metrics_per_client_per_facility (id BIGSERIAL PRIMARY KEY, created_at timestamp default CURRENT_TIMESTAMP, metric_registry_uuid UUID references metrics.metric_registry(id), -- FK of meta_info_table text -- UUID Is used instead of metric_name unique because user can insert -- value in metric table of other metric by mistake which would lead to wrong calulation of existing as well as new metric metric_id text not null, -- Unique name to each metric combination of metric_registry_uuid+client+facility+start_date+Granularity metric_column_id text not null, -- Unique name to each metric column combination of metric_registry_uuid_client+facility+start_date+end_date+metric_column_name+Granularity date_type text not null, date_value timestamp not null, start_date timestamp not null, end_date timestamp not null, -- // #TODO Could be replaced by granularity granularity interval not null, -- 1h, 2h, 1d, 7d client_id text, facility_id text, metric_column_name text not null, -- ALWAYS PUT METRIC COLUMN NAME IN lower_case_with_underscore metric_column_value text, metric_group text -- Entity/Group/Whatever the metric is calculating ,usually the main table entity it queris ); -- TODO To be partioned later on start_date, facility_id and client_id Create a new metric definition [One Time Registration] #  -- This query returns the metric_uuid for the registered metric. It's very important and will be used in below queries. INSERT INTO metrics.metric_registry (metric_name,created_by_email,created_for,used_by,stakeholder_emails,description,reporting_query_id,should_run) VALUES ('b2b-center-connect2','kartik.modi@delhivery.com','test_usage','{stm}','{stm@delhivery.com}','test description','101','t') RETURNING id; Insert Data in Metric Table - The query to be Scheduled by query authors #  Inserting the result of the metric in mat_metric table\nWITH tmp_variables AS ( SELECT CURRENT_DATE + interval '1' DAY AS dt ) INSERT INTO metrics.mat_metrics_per_client_per_facility( metric_registry_uuid ,metric_id ,metric_column_id ,date_type ,date_value ,start_date ,end_date ,granularity ,client_id ,facility_id ,metric_column_name ,metric_column_value ,metric_group ) -- 4d5beeef-f1a3-4263-b044-695a307f2370 is the metric_uuid which is received by above query SELECT '4d5beeef-f1a3-4263-b044-695a307f2370' ,'4d5beeef-f1a3-4263-b044-695a307f2370' || '|' || COALESCE(center_code,'NULL') || '|' || CURRENT_DATE || '|' || CURRENT_DATE || '|' ||'1d' ,'4d5beeef-f1a3-4263-b044-695a307f2370' || '|' || COALESCE(center_code,'NULL') || '|' || CURRENT_DATE || '|' || CURRENT_DATE || '|'|| metric_column_name ||'|'||'1d'	,'cd' ,CURRENT_DATE ,CURRENT_DATE ,CURRENT_DATE ,'1d' ,NULL ,center_Code ,metric_column_name ,metric_column_value ,'b2b' FROM ( SELECT * -- ALWAYS PUT metric_column_name in lower case ONLY ,unnest(array ['status','total','_sample_wbn','max']) AS metric_column_name ,unnest(array [status,total::text, _sample_wbn, max::text]) AS metric_column_value from ( --- YOUR STANDARD SELECT QUERY SELECT ocid AS center_code ,( CASE WHEN \u0026quot;cs.act\u0026quot; IN ( '+L' ,'+C' ,'\u0026lt;L' ,'\u0026lt;C' ) --(\u0026lt;L \u0026amp; \u0026lt;C) included to account for short shipments in RT status THEN 'connected' ELSE 'not_connected' END ) AS STATUS ,CURRENT_DATE -- - INTERVAL '1' day AS dt ,COUNT('*') AS total ,max(t4.wbn) AS _sample_wbn ,max(last_dispatched) AS \u0026quot;max\u0026quot; FROM ( SELECT action_date ,ocid ,oc ,\u0026quot;cs.act\u0026quot; ,\u0026quot;cs.sd\u0026quot; ,wbn ,last_dispatched ,action_center FROM ( SELECT action_date ,wbn ,\u0026quot;cs.ss\u0026quot; ,oc ,ocid ,\u0026quot;cs.sd\u0026quot; ,\u0026quot;cs.act\u0026quot; ,\u0026quot;cs.nsl\u0026quot; FROM ( SELECT action_date ,wbn ,\u0026quot;cs.ss\u0026quot; ,\u0026quot;cs.slid\u0026quot; ,oc ,ocid ,\u0026quot;cs.sd\u0026quot; ,\u0026quot;cs.act\u0026quot; ,\u0026quot;cs.dwbn\u0026quot; ,\u0026quot;cs.st\u0026quot; ,\u0026quot;cs.nsl\u0026quot; -- , row_number() OVER ( PARTITION BY wbn ORDER BY action_date DESC ) AS rn FROM raw_d.package__wbn__latest_d WHERE 1 = 1 -- and ad \u0026gt;= date_format((date_trunc('day', current_timestamp) - interval '1' day) - interval '00' minute, '%Y-%m-%d-%H') AND action_date \u0026gt;= (date_part('epoch'::TEXT, date_trunc('day', current_timestamp) - '330 minute'::interval) * 1000::DOUBLE PRECISION * 1000::DOUBLE PRECISION)::BIGINT -- and ad \u0026lt;= date_format((date_trunc('day', current_timestamp) - interval '0' day) - interval '00' minute, '%Y-%m-%d-%H') AND \u0026quot;date.mnd\u0026quot; \u0026gt;= (date_trunc('day', current_timestamp) - interval '0' day) - interval '330' minute -- and date_mnd \u0026lt;= (date_trunc('day', current_timestamp) - interval '0' day) - interval '330' minute AND cl = 'Delhivery E POD' ) AS t1 -- WHERE rn = 1 ) t2 LEFT JOIN ( SELECT action_center ,max(TO_DATE('1970-01-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS') + (action_date / 1000000) * INTERVAL '1' second) AS last_dispatched FROM raw_d.trips_d WHERE 1 = 1 -- and ad \u0026gt;= date_format((date_trunc('day', current_timestamp) - interval '1' day) - interval '00' minute, '%Y-%m-%d') AND action_date \u0026gt;= (date_part('epoch'::TEXT, date_trunc('day', current_timestamp) - '330 minute'::interval) * 1000::DOUBLE PRECISION * 1000::DOUBLE PRECISION)::BIGINT -- and ad \u0026lt;= date_format((date_trunc('day', current_timestamp) - interval '0' day) - interval '00' minute, '%Y-%m-%d') -- AND action_date \u0026gt; cast(to_unixtime((date_trunc('day', current_timestamp) - interval '0' day) - interval '330' minute) as bigint)*1000000.0 -- AND action_date \u0026gt; cast(to_unixtime((date_trunc('day', current_timestamp) - interval '0' day) - interval '330' minute) as bigint)*1000000.0 AND action_performed = 'Departed' GROUP BY action_center ) t3 ON ocid = action_center ) t4 WHERE ( action_date \u0026lt; extract(epoch FROM ( CASE WHEN last_dispatched IS NULL OR last_dispatched \u0026gt; ( ( SELECT dt FROM tmp_variables ) - INTERVAL '10' hour - INTERVAL '30' minute ) THEN ( SELECT dt FROM tmp_variables ) - INTERVAL '13' hour - INTERVAL '30' minute WHEN last_dispatched \u0026lt; ( ( SELECT dt FROM tmp_variables ) - INTERVAL '18' hour - INTERVAL '30' minute ) THEN ( SELECT dt FROM tmp_variables ) - INTERVAL '37' hour - INTERVAL '30' minute ELSE last_dispatched - INTERVAL '180' minute END )) * 1000000 ) GROUP BY 1 ,2 --- YOUR STANDARD SELECT QUERY END ) A )B; SELECTing Data from Mat Metric Table - (To be created for every metric) #  The structure of getting metric from mat_metrics_per_client_per_facility is gonna be the same for all metric with some changes like column names output, metric_uuid, filters etc.\n For convention naming see datamarts-creation-guidlines\n -- change output column, function name \u0026amp; arguments CREATE OR REPLACE FUNCTION stm.stm__b2b_center_connect_for_facility_ids__for_range__at_day_level(facility_ids_filter text[],start_date_filter date, end_date_filter date) RETURNS TABLE(metric_id text ,facility_id text,date_type text,date_value timestamp,sum bigint,total bigint,_sample_wbn text,max bigint) AS -- BOILER_PLATE_CODE $func$ BEGIN RETURN QUERY EXECUTE format($execute$ -- typecast the output as per user expectation. The database returns metric column value in text only -- if there's LIVE metric datamart, output of both LIVE and MATERIALIZED should be compatible so the user can do UNION select metric_id,facility_id ,date_type,date_value,sum::bigint,total::bigint,_sample_wbn text,max::bigint from crosstab( $$ SELECT metric_id,facility_id,date_type,date_value,metric_column_name,metric_column_value from ( select metric_id,facility_id,date_type,date_value, metric_column_name,metric_column_value, row_number() over ( partition by metric_column_id order by created_at desc) as rnum from metrics.mat_metrics_per_client_per_facility -- Use the metric_id created in above where metric_registry_uuid = '4d5beeef-f1a3-4263-b044-695a307f2370' -- Use filters passed from enduser to get needed metric only and (facility_id = any (%L) ) and start_date \u0026gt;= %L start_date \u0026lt; %L -- Getting latest version of that metric. Incase the same metric is calculated multiple times in the same time frame ) a where rnum = 1 $$ --Second argument crosstab function starts , $$VALUES ('status'), ('total'),('_sample_wbn'),('max') $$) AS -- Defining type ct(metric_id text, facility_id text, date_type text, date_value timestamp, status text, total text, _sample_wbn text, max text); -- Format function ends here $execute$ -- Passing filters of end-user to the query before running ,facility_ids_filter, start_date_filter, end_date_filter); -- BOILER_PLATE_CODE END; $func$ LANGUAGE plpgsql VOLATILE; Usage : To be queried from application code #  select * from stm.stm__b2b_center_connect_for_facility_ids__for_range__at_day_level('{IN110051A1C}','2021-03-23','2021-03-28'); "}),a.add({id:10,href:'/docs/docs/usage-guidelines/navigation-around-views-and-tables/',title:"navigation-around-views-and-tables",section:"Usage Guidelines",content:"Query to get table name from view name #  -- referenced_table_name will contain the tablename SELECT * FROM (SELECT u.view_schema AS schema_nameD, u.view_name, u.table_schema AS referenced_table_schema, u.table_name AS referenced_table_name, v.view_definition FROM information_schema.view_table_usage u JOIN information_schema.views v ON u.view_schema = v.table_schema AND u.view_name = v.table_name WHERE u.table_schema NOT IN ('information_schema', 'pg_catalog') --- CHANGE VIEW NAME HERE AND u.view_name= '{view_name}' ORDER BY u.view_schema, u.view_name) AS VIEW_DETAILS; -- Source https://dataedo.com/kb/query/postgresql/list-tables-used-by-a-view Query to get move around in partition table tree hierarchy #  SELECT * FROM pg_partition_root(table_name); SELECT * FROM pg_partition_ancestors(table_name); SELECT * FROM pg_partition_tree(table_name); "}),a.add({id:11,href:'/docs/docs/usage-guidelines/api/offset-management-apis-usage/',title:"Offset management",section:"Usage Guidelines",content:"Spark-Structured Streaming Offset Management APIs #  spark-structured streaming has it\u0026rsquo;s own offset management it does not\nsupport storing consumed offset information in consumer group in kafka.\nIt uses checkpoint location which you provide. we are using s3 as\ncheckpoint location. Currently there is no APIs in spark-structured\nstreaming using which we can get information of currently consumed\noffsets or reset offsets for current pipeline. We created APIs which\nwill do all of this work.\nOffset Management APIs #  1.get currently consumed offsets by pipeline:\nThe API gives information of currently consumed offset by pipeline.\nRequest:\ncurl -X GET -H \u0026quot;Accept:application/json\u0026quot; -H \u0026quot;Content-Type:application/json\u0026quot; localhost:9090//v1/getCurrentOffsets/?pipelineName=orion_transaction  Response:\n{ \u0026quot;Orion.Transaction.info\u0026quot;: { \u0026quot;0\u0026quot;: 873340, \u0026quot;1\u0026quot;: 884664, \u0026quot;2\u0026quot;: 879160, \u0026quot;3\u0026quot;: 866873, \u0026quot;4\u0026quot;: 894115, \u0026quot;5\u0026quot;: 880421, \u0026quot;6\u0026quot;: 868078, \u0026quot;7\u0026quot;: 885777, \u0026quot;8\u0026quot;: 876230 } }  2.get topic details:\nThe API gives detail of given topic like starting offset and ending offset of each partition.\nRequest :\n curl -X GET -H \u0026quot;Accept:application/json\u0026quot; -H \u0026quot;Content-Type:application/json\u0026quot; localhost:9090//v1/getTopicDetails/?topicName=Orion.Transaction.info  Response:\n{ \u0026quot;name\u0026quot;: \u0026quot;Orion.Transaction.info\u0026quot;, \u0026quot;partitions\u0026quot;: { \u0026quot;0\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 873340, \u0026quot;lastOffset\u0026quot;: 898067 }, \u0026quot;1\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 884664, \u0026quot;lastOffset\u0026quot;: 909803 }, \u0026quot;2\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 879160, \u0026quot;lastOffset\u0026quot;: 904819 }, \u0026quot;3\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 866873, \u0026quot;lastOffset\u0026quot;: 892826 }, \u0026quot;4\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 894115, \u0026quot;lastOffset\u0026quot;: 917581 }, \u0026quot;5\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 880421, \u0026quot;lastOffset\u0026quot;: 905385 }, \u0026quot;6\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 868078, \u0026quot;lastOffset\u0026quot;: 893108 }, \u0026quot;7\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 885777, \u0026quot;lastOffset\u0026quot;: 908980 }, \u0026quot;8\u0026quot;: { \u0026quot;firstOffset\u0026quot;: 876230, \u0026quot;lastOffset\u0026quot;: 902482 } } }  3.reset offset of all partitions to earliest:\nThe API resets offset of the consumer to the earliest offset of each partition.\nRequest:\ncurl -X GET -H \u0026quot;Accept:application/json\u0026quot; -H \u0026quot;Content-Type:application/json\u0026quot; localhost:9090//v1/resetOffsetOfAllPartitionsToEarliest/?pipelineName=orion_transaction  Response:\n{ \u0026quot;Orion.Transaction.info\u0026quot;: { \u0026quot;0\u0026quot;: 873340, \u0026quot;1\u0026quot;: 884664, \u0026quot;2\u0026quot;: 879160, \u0026quot;3\u0026quot;: 866873, \u0026quot;4\u0026quot;: 894115, \u0026quot;5\u0026quot;: 880421, \u0026quot;6\u0026quot;: 868078, \u0026quot;7\u0026quot;: 885777, \u0026quot;8\u0026quot;: 876230 } }  4.reset offset of all partitions to latest:\nThe API resets offset of the consumer to the latest offset of each partition.\nRequest:\ncurl -X GET -H \u0026quot;Accept:application/json\u0026quot; -H \u0026quot;Content-Type:application/json\u0026quot; localhost:9090//v1/resetOffsetOfAllPartitionsToLatest/?pipelineName=orion_transaction  Response:\n{ \u0026quot;Orion.Transaction.info\u0026quot;: { \u0026quot;0\u0026quot;: 898071, \u0026quot;1\u0026quot;: 909810, \u0026quot;2\u0026quot;: 904827, \u0026quot;3\u0026quot;: 892828, \u0026quot;4\u0026quot;: 917590, \u0026quot;5\u0026quot;: 905389, \u0026quot;6\u0026quot;: 893126, \u0026quot;7\u0026quot;: 908987, \u0026quot;8\u0026quot;: 902492 } }  5.reset offset of partition:\nThe API resets offset of the consumer to the specified offset at partition level. We specify all number of partitions that exist in the topic. This api is useful when we want to want to reset offset to a custom pre-calculated offset for each partition.\nRequest:\ncurl -X GET -H \u0026quot;Accept:application/json\u0026quot; -H \u0026quot;Content-Type:application/json\u0026quot; localhost:9090//v1/resetOffsetOfPartitions/?pipelineName=orion_transaction -d @offset.json offset.json file content { \u0026quot;Orion.Transaction.info\u0026quot;: { \u0026quot;0\u0026quot;: 0, \u0026quot;1\u0026quot;: 895788, \u0026quot;2\u0026quot;: 890201, \u0026quot;3\u0026quot;: 877647, \u0026quot;4\u0026quot;: 904420, \u0026quot;5\u0026quot;: 891558, \u0026quot;6\u0026quot;: 879249, \u0026quot;7\u0026quot;: 896014, \u0026quot;8\u0026quot;: 887096 } }  Response:\n{ \u0026quot;Orion.Transaction.info\u0026quot;: { \u0026quot;0\u0026quot;: 0, \u0026quot;1\u0026quot;: 895788, \u0026quot;2\u0026quot;: 890201, \u0026quot;3\u0026quot;: 877647, \u0026quot;4\u0026quot;: 904420, \u0026quot;5\u0026quot;: 891558, \u0026quot;6\u0026quot;: 879249, \u0026quot;7\u0026quot;: 896014, \u0026quot;8\u0026quot;: 887096 } }  "}),a.add({id:12,href:'/docs/docs/developer/outcomes-limitations-support/',title:"outcomes limitations support",section:"Developer",content:"Outcomes #   package_meta - Standard package data  Used when filter on action_date (epoch in microseconds)   Use of action_date column as a preliminary first-level basic filter is required \u0026amp; mandatory.\n  package_multidimensional - Multi-dimensional package data which is supposed to be used only when action_date filter is NOT possible. Used to query package data using ANY BUT AT LEAST one of the following -  cs.sd -\u0026gt; scan timestamp of package scan cd -\u0026gt; package\u0026rsquo;s created timestamp wbn -\u0026gt; waybillnumber which was either created / updated in last 6 months  All the wbns of a particular client ends up a in a single partition so querying for list of wbns of a client it\u0026rsquo;s faster.\n    This is first level preliminary basic filter. As of now other data querying sources support only action_date / ad.\n  When query uses client key (cl) as a filter, planning and execution time is significantly reduced. Can add more first level preliminary filters at a trade-off to space / IO if highly needed. eg. cs.ud, date_mnd, cs_uid, ad  Query level use-cases #   ANY query which is NOT I/O bound. What that means is the query should return a certainly limited number of records. A ballpark number is \u0026lt;1000.  Please don\u0026rsquo;t get confuse with number of records. The query can scope hundreds of thousands of records but with proper filters.   Since it\u0026rsquo;s still a RDBMS, it can serve hundreds of concurrent (but limited parallel) query executions.  Scalability #   Able to server data of packages created in last 6 months at 3x the 2020\u0026rsquo;s peak package scale (though limited by storage cost)  Caveats and Limitations #   Significant Increased maintenance cost -\u0026gt; Have to main 2 different tables for 1 pipeline and update the view also. Keep the sync b/w 2 tables i.e. pipeline partitioned table and it\u0026rsquo;s global table. We cannot use detech and drop partitions. Cleanups has to be still on DELETEs. Probably some rewrite in dbcleanup For every table need to write some implementations, brainstorming, use-case evaluations.  Bloating #   The bloating is concern in mainly global table indexes which needs proper cleanup regularly The partitioned table and it\u0026rsquo;s indexes will have significantly low bloating  Probable Concerns #   Implications of Trigger and impact on scale  Perks and Benefits #   With postgres we get a posix compliant SQL with extremely wide library of functions. People are already comfortable in querying from postgres  What about other pipeline integration? #  The bag table will be first level partitioned by created date and then second level by bs. The reason is that bag is joined to other tables based on bs. Similarly, trips will also be partitioned on created date first and then on trip_uuid at second level.\nInfrastructure limitations as of 24 Dec 2020 #   We are heavily dependent on pg v12. Partitioning improvements(pruning, join, aggregates) and number of partitions chosen. We are looking forward to upgrade for pg v13 too because of more partitioning features. See this and v13 is in preview on RDS Aurora is still on v11.x so no scope for converting to aurora for a long time. pg_partman and pg_cron will be available in RDS with pgv13 soon but no commitments for aurora No disk compression extension of postgres in RDS neither zfs FS is supported. No multiple disk support in RDS. So all partitions have same storage IO bandwidth.  Resource Allocation Strategies #   Priorities set work mem at user level ALTER USER johndoe WITH CONNECTION LIMIT 2; https://towardsdatascience.com/how-to-handle-privileges-in-postgresql-with-specific-use-case-and-code-458fbdb67a73  Future Scope #   Mandate usage of any of preliminary first level filter columns (seems doesn\u0026rsquo;t exist natively) Parallel_workers in postgres create table for new partitions (exploration) Cluster based on action_date this Explore privelegs and control in depth Incorporate Platform use-cases RDS Proxy for pooling Cluster by on action_Date / cs_ud Explore compaction to latest event in the batch before inserting/updating Splitting of table into multiple tables based on priority of columns (kind of columnar storage) - Suggested by AWS Since we only need partitioned table for query on action_date filter we can avoid replicating global table to niche replicas  Exploration #   https://pghintplan.osdn.jp/pg_hint_plan.html pg_proctab pg_repack or better pg_stat_statements pglogical 2 pgrowlocks pgstattuple plv8 tablefunc tsm_system_rows tsm_system_time uuid-ossp amcheck postgresql-hll  Costing #  Refer this\n"}),a.add({id:13,href:'/docs/docs/usage-guidelines/programmatic-access-and-integration/',title:"programmatic access and Integration",section:"Usage Guidelines",content:"Guidelines for programmatic Integration with DMDB #    Use only the user provided to you for your specific application. Even if you have any other credentials like developers credentials, some other application\u0026rsquo;s credentials, etc.; dont use them. Ask for new credentials for each new application/dashboard.\n  Connection Timeout (300ms) - Use this while making the connection to DB. If the connection fail within the timeout move further, log error (raise alarms). The default is unlimited time, so application can get stuck when DB is down if this is not specified.\n Retry continuously for 10 times for connection if you received ~ConnectionExhaust, ~DNSNotResolvedException, ~AddressUnavailable, ~ConnectionRefused etc. without any backoff.    Query Timeout (500ms) - Use this while querying data from DB. Ideally the queries should run in around ~150ms so this is a much safer value.\n Retry Logic - If the query takes \u0026gt;500ms , then because of timeout you should get ~QueryTimeoutException. Handle this and retry the query now with a timeout of 3000ms. Retry only once.    General Retry in case of other exception - In case of exceptions like ~ConnectionExhaust, ~DBObjectNotAvailable, ~DBNotAvailable, ~DBShuttingDown which could be transient in nature, use retry mechanism with a backoff. The configuration of this depends on your use-case and situations. The suggested values is 3 times retry with a increasing backoff starting from 1000ms. Please don\u0026rsquo;t retry for every Exception(blanket) eg. ~SyntaxIssueException, ~PermissionException etc. because these are not transient and cannot be fixed without manual intervention.\n  If your application needs to fire multiple queries for a single page load, then start executing all queries in parallel. The database serves can serve of several concurrent requests at once.\n  So when the dashboard page starts to open it fires all the queries for the selected facilities/group of facilities for a day/range of days.\nFor example,\nif there are 5 metrics being shown on a page on the dashboard, and from DB th e respective query execution time is 120ms, 150ms, 30ms, 68ms, 190ms then, the total dashboard page loading time will be approx equal to = maximum query execution time among all queries - 190ms in this case + API compute time + network latency = ~220ms\n    Always use datamart-db.delhivery.com endpoint for connecting to DB. This is a replica machine and all read queries from application/dashboard backend should be fired to this endpoint only.\n  For connection pooling, use connection pooling if possible. There are libraries available for pooling at application level in most languages. Each user is alloted a 25 connections to start with. THis can be increased on case-to-case basis.\n   To get credentials for a new dashboard/application backend please contact dg-tech@delhivery.com\n "}),a.add({id:14,href:'/docs/docs/developer/roadmap/',title:"Roadmap",section:"Developer",content:"Roadmap with Priority , Effort \u0026amp; Timeline #     Objective Priority Effort Timeline Status     Deadlock Prevention Logic P2 3d 30 May    Refactoring of configuration main P2 2d 15 May    New Integration Document P3 1d 12 May In progress   Health Check API P0 5d 7 May In progress   Architecture: * Exploration Documents * Spark documentation - P1 * Videos Links P1 10d Jun    Application tuning for throughput increase P3 3d Jun    Addition of meta columns like topic_name, offset, partitions etc. P2 1d 30 May    Fix historical dump issue for package array. P0 5d 12 May In progress   Dump all historical data in DMDB. P0 4d 18 May In progress   Access control management HLD. P0 5d 18 May In Progress   Migration of Timer based DC Dashboard to Native DMDB P4 3d Jun    Configurable Start Offset P1 1d 22 May    Setup Documentation P3 2d Jun    API Documentation P3 4d Jun    DMDB Dev Env AutoKill Setup - With TerminateOn P4 10d July    Website Publish P2 2d May End    Arun Pandey IOPS Control and Tuning DB P3 2d May End    Checkstyle P4 2d July    Permission Suite Check - If there\u0026rsquo;s everythign configured right framework P5 10d August    Drop and create view - Atomically P2 3d May End    Make permission consistent for future DB objects P1 3d Done    inconsistent schema issue key missing prakash - 2 schemas for same package thing P1 1d Done     Extra Notes:\n https://databricks.com/session_na20/how-adobe-does-2-million-records-per-second-using-apache-spark https://www.one-tab.com/page/haktEgztToy_U2v2pDDkVg | 1f  "})})()